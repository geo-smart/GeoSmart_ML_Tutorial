{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Model Development and Parameter Tuning\n",
    "\n",
    "\n",
    "\n",
    "The model pipeline consists of a hybrid-ML technique, utilizing two separate and optimized frameworks outlined in the figure below.Hybrid approaches in ML modeling, thogh more complex, show improved accuracy over traditional single algorithm models. The integration of multiple algorithms allows for more robust models and methods of feature set derivation.\n",
    "Here, we apply a hybrid approach of first utilizing Gradient Boosted decision Trees to select an optimal model feature space, and follow it with a supervised MLP regression model. Output of the hybrid model is 1-km gridded SWE estimates throughout the study area.\n",
    "\n",
    "<img align = 'center' src=\"./Images/SWE_model_workflow_V2.png\" alt = 'drawing' width = '400'/>\n",
    "\n",
    "### Feature Selection With Gradient Boosted Decision Trees\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary dependencies and path to the parent directory of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tables\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump\n",
    "import sklearn\n",
    "import graphviz\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from platform import python_version\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import cv\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance as plot_importance_XGB\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance as plot_importance_LGBM\n",
    "\n",
    "print(pd.__version__) # should be 1.3.0\n",
    "print(sklearn.__version__) # should be 0.24.1\n",
    "print(python_version()) \n",
    "\n",
    "os.chdir('Your Model Directory Here')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define the Region, here we are using the Northern Colorado Rockies region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load regionalized geospatial data\n",
    "\n",
    "### define regions\n",
    "Region_list = ['N_Co_Rockies']\n",
    "\n",
    "### Load H5 train files into dictionary\n",
    "RegionTrain= {}\n",
    "for region in Region_list:\n",
    "    RegionTrain[region] = pd.read_hdf('Provided_Data/Final_Training_DF.h5', region)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define a hyperparameter grid dictionary object to be optimized later on during cross-validation splittling. The ranges here have been tuned to provide good model performance, however you should change the values around and try additional hyperparameters to see how it affects performance and efficieny. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameter grid\n",
    "\n",
    "LGBM_param_grid = {\n",
    "    \"max_depth\": list(range(3,30,4)),\n",
    "    \"num_leaves\": list(range(5,55,10)),\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9],\n",
    "    \"n_estimators\": list(range(300,700,100))  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our Gradient Boosting Decision Tree Model. First, initializing the class parameters and creating the make_dataframe method. This method formats a dataframe of the data for use in the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REGRESSOR(object):\n",
    "    \"\"\"\n",
    "    Regression optimization model class.\n",
    "    Args:\n",
    "        target (str): target to be modeled.\n",
    "        data (dict): Dictionary of dataframes containing training/testing data\n",
    "        estimator (model object): type of model to be fit.\n",
    "        param_grid (dict): hyperparameter grid\n",
    "        model (fit XGBRegressor, LGBMRegessor, etc..): the fit model object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, data, estimator=None, param_grid=None, model=None):\n",
    "        self.target = target\n",
    "        self.data = data\n",
    "        self.estimator = estimator \n",
    "        self.param_grid = param_grid\n",
    "        self.model = model\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_dataframe(region, data):\n",
    "        \"\"\"Dataframe manipulation.\"\"\"\n",
    "        \n",
    "        df = data.get(region)\n",
    "        \n",
    "        ### replace -9999s with nans\n",
    "        df=df.replace([-9999, -9999.99], np.nan)     \n",
    "        \n",
    "        ### create new index 0 - len(df)\n",
    "        df= df.reset_index(drop=False)\n",
    "        df =df.rename(columns={\"index\": \"cell_id\"})\n",
    "        if 'Date' in df.columns:\n",
    "            df = df.drop(columns=['Date']) \n",
    "        \n",
    "        ###create indexed series of cell ids\n",
    "        id_map = df.pop('cell_id').to_frame()\n",
    "        id_map['Long'] = df['Long']\n",
    "        id_map['Lat'] = df['Lat']\n",
    "        id_map['WYWeek'] = df['WYWeek']\n",
    "        id_map['elevation_m'] = df['elevation_m']\n",
    "\n",
    "        ### shuffle dataframe\n",
    "        df = df.sample(frac=1, random_state=1234)\n",
    "        \n",
    "        ### replace special character ':' with '__' \n",
    "        df = df.rename(columns = lambda x:re.sub(':', '__', x))\n",
    "        \n",
    "        return df, id_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the fit_ method. This scales the dataframe from the previous step with a Min-Max scaler and splits the data to 25-75 training testing split. A cross-vaildation grid search with 5 folds is then performed on the data for the LGBM_param_grid. We select the best parameters using the .best_params_ parameter and fit the model to those parameters.\n",
    "\n",
    "The fit model now undergoes the first feature selection with the .Feature_Selection method we define in the next block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     def fit_(self, region, cv=5):\n",
    "            \"\"\"Gridsearch for Parmas, Fit model, and save as instance attribute.\"\"\"\n",
    "            start_time = time.time()\n",
    "\n",
    "            ### Dataframe engineering step\n",
    "            df, id_map = self.make_dataframe(region, self.data)\n",
    "\n",
    "            y = df.pop(self.target)\n",
    "\n",
    "\n",
    "            ###normalize features\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled = scaler.fit_transform(df)\n",
    "            df = pd.DataFrame(scaled, columns = df.columns)\n",
    "            #save scaler data here\n",
    "            pickle.dump(scaler, open(os.getcwd()+'/Model/Model_Features/'+'LGBM_'+region+'_scaler.pkl', 'wb'))\n",
    "\n",
    "            X = df\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)\n",
    "\n",
    "\n",
    "            ### Fit CV to define optimal hyperparmaeters\n",
    "            gs = GridSearchCV(\n",
    "                self.estimator(),\n",
    "                self.param_grid,\n",
    "                cv=5,\n",
    "                n_jobs=-1,\n",
    "                return_train_score=True,\n",
    "                refit=False\n",
    "            )\n",
    "\n",
    "            gs.fit(X_train, y_train)\n",
    "\n",
    "            self.best_score = gs.best_score_\n",
    "            self.param_grid = gs.best_params_\n",
    "            print('Best Score:', gs.best_score_)\n",
    "            print('Best Params:', gs.best_params_)\n",
    "\n",
    "\n",
    "            ### Fit the estimator model with the optimal params\n",
    "            self.model = self.estimator(**self.param_grid)\n",
    "\n",
    "            X_train_rfe, X_test_rfe, y_train_rfe, y_test_rfe = self.Feature_selection(X_train, X_test, y_train, y_test)\n",
    "\n",
    "            c_time = round(time.time() - start_time,2)\n",
    "            print('Calibration time', round(c_time), 's')\n",
    "\n",
    "            return X_train_rfe, X_test_rfe, y_train_rfe, y_test_rfe, id_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the recursive feature elimination. Using the sklearn RFECV() function, a minimum of 1 optimal features are chosen for the region based on a 5-fold cross validation. The less informing features are ten dropped from the train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Feature_selection(self, X_train, X_test, y_train, y_test):\n",
    "            \"\"\"Identify and and fit model with optimal features \"\"\"\n",
    "            ### Define RFE with CV model\n",
    "            min_features_to_select = 1  # Minimum number of features to consider\n",
    "            rfecv = RFECV(\n",
    "                estimator=self.model,\n",
    "                step=1,\n",
    "                cv=5,\n",
    "                n_jobs=-1,\n",
    "                min_features_to_select=min_features_to_select,\n",
    "            )\n",
    "\n",
    "            rfecv.fit(X_train, y_train)\n",
    "\n",
    "            ### dataframe of optimal features, remove non-optimal features from train/test data \n",
    "            feat = {'Features_raw': X_train.columns}\n",
    "            RFECV_Feat = pd.DataFrame(data=feat)\n",
    "            RFECV_Feat['Rank']= rfecv.ranking_\n",
    "            RFECV_Feat['Selected']= rfecv.support_\n",
    "            RFECV_Feat_opt = RFECV_Feat[RFECV_Feat['Selected']==True]\n",
    "            RFECV_Feat_opt = RFECV_Feat_opt['Features_raw']\n",
    "            RFECV_Feat_opt.columns=['Features_opt']\n",
    "            print('The optimal features are: ', list(RFECV_Feat_opt))\n",
    "            ### use optimal features for final fit data\n",
    "            X_train=X_train[list(RFECV_Feat_opt)]\n",
    "            X_test=X_test[list(RFECV_Feat_opt)]\n",
    "\n",
    "            ### fit model with optimal parmas & optimal # of featuress\n",
    "            self.model = self.estimator(**self.param_grid)\n",
    "            self.model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "            ### Plot number of features vs. cross-validation scores\n",
    "            plt.figure()\n",
    "            plt.xlabel(\"Number of features selected\")\n",
    "            plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "            plt.plot(range(min_features_to_select, len(rfecv.cv_results_[\"mean_test_score\"]) + min_features_to_select),\n",
    "                rfecv.cv_results_[\"mean_test_score\"])\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "            ### Revert special character '__' with ':'\n",
    "            X_train = X_train.rename(columns = lambda x:re.sub(r\"(_)\\1+\", ':', x))\n",
    "            X_test = X_test.rename(columns = lambda x:re.sub(r\"(_)\\1+\", ':', x))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we may define a .predict_ method on the Decision tree model to predict SWE against the test locations. We will ultimately not use this prediction as the Nueral Net proves more robust, however it can be insightful for comparing the diffent algorithm performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_(self, X_test):\n",
    "            \"\"\"Generate model predictions.\"\"\"\n",
    "\n",
    "            preds = (self.model.predict(X_test))\n",
    "\n",
    "            ### correct negative predictions\n",
    "            preds[preds <0]=0\n",
    "\n",
    "            return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather tahn indivdually call each method, we can create a new method, called .Batch_Train that will run through the steps we just defined. This will take in our paramater objects and return a pickle file of the optimal features (\"opt_features_final.pkl) for the region that will be used to train the Neural Net in the following steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Batch_Train(target, data, algorithm, parameter_grid, Region_list):\n",
    "        \"\"\"\n",
    "        Batch train regression model and produce testing prediction.\n",
    "        Args:\n",
    "            algorithm (model object): Regressor to be fit (XGBRegressor, LGBMRgressor)\n",
    "            param_grid (dict): Hyperparameter grid\n",
    "            Region_list (list): List of regions to be evaluated\n",
    "        \"\"\"\n",
    "\n",
    "        split_dict={}\n",
    "        prediction_dict={}\n",
    "        features_dict = {}\n",
    "        ### define prefix for path based on algorithm desired\n",
    "        if algorithm == XGBRegressor:\n",
    "            path_name = 'XGB'\n",
    "            from xgboost import plot_importance\n",
    "        elif algorithm == LGBMRegressor:\n",
    "            path_name = 'LGBM'\n",
    "            from lightgbm import plot_importance\n",
    "        else:\n",
    "            raise ValueError(\"Algorithm not recognized. Must be XGBRegressor or LGBMRegressor.\")\n",
    "\n",
    "        for region in Region_list:\n",
    "            ###Instantiate Model\n",
    "            region_model = REGRESSOR(target=target, data=data, estimator=algorithm, param_grid=parameter_grid)\n",
    "\n",
    "            ###Fit region model and add dictionary entry for region test-train data, save fit model to file.\n",
    "            ###Dictformat is, split_dict{'region': [X_train, X_test, y_train, y_test, cell_id]}\n",
    "            print(region,':')\n",
    "            split_dict[region] = region_model.fit_(region)\n",
    "            print('Saving Model')\n",
    "            pickle.dump(region_model, open(os.getcwd()+\"/Model/Model_Training/LGBM_\"+region+\".pkl\", \"wb\"))\n",
    "\n",
    "            ###Generate predictions and add to prediction dictionary,\n",
    "            ###Dict format is, prediction_dict{'region': [preds]}\n",
    "            prediction_dict[region] = region_model.predict_(split_dict.get(region)[1])\n",
    "\n",
    "            ### plot predictions if desired\n",
    "            fig, ax =plt.subplots()\n",
    "            sns.set(style=\"whitegrid\")\n",
    "            plt.scatter(split_dict.get(region)[3], prediction_dict.get(region), marker='.',s=100, color = 'b')\n",
    "            plt.plot([0,(prediction_dict.get(region).max()+10)], [0,(prediction_dict.get(region).max()+10)], ls=\"--\", c=\".1\")\n",
    "            plt.xlim([0, (split_dict.get(region)[3].max()+5)])\n",
    "            plt.ylim([(prediction_dict.get(region).min()-1),(prediction_dict.get(region).max()+5)])\n",
    "            ax.set_title(region)\n",
    "            ax.set_xlabel(\"Truth\")\n",
    "            ax.set_ylabel(\"Prediction\")\n",
    "            plt.show()\n",
    "            rmse = mean_squared_error(split_dict.get(region)[3], prediction_dict.get(region), squared=False)\n",
    "            print(region, \"RMSE:\", (round(rmse,3)))\n",
    "\n",
    "            ###Optimal Features dictionary \n",
    "            features_dict[region] = split_dict[region][0].columns\n",
    "\n",
    "\n",
    "        ###Save output dictionaries to file\n",
    "        pickle.dump(split_dict, open(os.getcwd()+\"/Model/Model_Features/\"+path_name+\"_split_dict_final.pkl\", \"wb\"))\n",
    "        pickle.dump(prediction_dict, open(os.getcwd()+\"/Model/Model_Features/\"+path_name+\"_prediction_dict_final.pkl\", \"wb\"))\n",
    "        pickle.dump(features_dict, open(os.getcwd()+\"/Model/Model_Features/Optimal_Features.pkl\", \"wb\"))\n",
    "\n",
    "        return split_dict, prediction_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "splits_LGBM , preds_LGBM = Batch_Train('SWE', RegionTrain, LGBMRegressor, LGBM_param_grid, Region_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can enter our parameter values into the Batch_Train method and have the model define our optimal features. These feature are crucial for training the Neural Network, covered in the next [Chapter](./training.ipynb).. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c446eef832ec964573dc49f36fd16bdbed40cbfbefbf557bc2dc78d9e7968689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
